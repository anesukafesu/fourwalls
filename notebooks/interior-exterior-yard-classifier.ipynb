{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8507898,"sourceType":"datasetVersion","datasetId":5078468}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Objective\n\n- Building a relatively-lightweight CNN to classify images into either interior or exterior images","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\n\n\nDATA_DIR = \"/kaggle/input/interior-exterior-scene-classification\"\nIMG_SIZE = 224\nBATCH_SIZE = 32\nEPOCHS = 20\n\n# Load image paths and labels\nimage_paths = []\nlabels = []\n\nlabel_map = {\"Interior\": 0, \"Exterior\": 1}\n\nfor label_name in [\"Interior\", \"Exterior\"]:\n    folder_path = os.path.join(DATA_DIR, label_name)\n    for fname in os.listdir(folder_path):\n        if fname.endswith(\".png\"):\n            image_paths.append(os.path.join(folder_path, fname))\n            labels.append(label_map[label_name])\n\n# Train-test split\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n)\n\n# Preprocessing function\ndef load_and_preprocess(path, label):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n    image = image / 255.0  # Normalise to between 0 and 1\n    return image, label\n\n# Build TensorFlow Datasets\ntrain_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\nval_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n\ntrain_ds = train_ds.map(load_and_preprocess).shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\nval_ds = val_ds.map(load_and_preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\n# Build base model (MobileNetV2 backbone)\nbase_model = tf.keras.applications.MobileNetV2(\n    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    include_top=False,\n    weights=\"imagenet\"\n)\nbase_model.trainable = False\n\n# Build the top model\ntop_model = models.Sequential([\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(128, activation=\"relu\"),\n    layers.Dropout(0.3),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n\n# Combine the base and the top\nmodel = models.Sequential([\n    base_model,\n    top_model\n])\n\n# Compile the model\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\n# Train the model\nhistory = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS\n)\n\n# Evaluate final accuracy\nloss, acc = model.evaluate(val_ds)\nprint(f\"\\nFinal validation accuracy: {acc:.4f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T12:43:40.643038Z","iopub.execute_input":"2025-07-04T12:43:40.643275Z","iopub.status.idle":"2025-07-04T12:52:31.099873Z","shell.execute_reply.started":"2025-07-04T12:43:40.643255Z","shell.execute_reply":"2025-07-04T12:52:31.099058Z"}},"outputs":[{"name":"stderr","text":"2025-07-04 12:43:42.360626: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751633022.563315      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751633022.624151      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-07-04 12:43:57.675045: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\nEpoch 1/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 918ms/step - accuracy: 0.7401 - loss: 0.5405 - val_accuracy: 0.9545 - val_loss: 0.1646\nEpoch 2/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 807ms/step - accuracy: 0.9548 - loss: 0.1220 - val_accuracy: 0.9394 - val_loss: 0.1491\nEpoch 3/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 830ms/step - accuracy: 0.9801 - loss: 0.0653 - val_accuracy: 0.9444 - val_loss: 0.1458\nEpoch 4/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 823ms/step - accuracy: 0.9944 - loss: 0.0428 - val_accuracy: 0.9545 - val_loss: 0.1401\nEpoch 5/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 804ms/step - accuracy: 0.9911 - loss: 0.0290 - val_accuracy: 0.9545 - val_loss: 0.1517\nEpoch 6/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 816ms/step - accuracy: 0.9907 - loss: 0.0255 - val_accuracy: 0.9646 - val_loss: 0.1497\nEpoch 7/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 825ms/step - accuracy: 0.9972 - loss: 0.0178 - val_accuracy: 0.9596 - val_loss: 0.1432\nEpoch 8/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 793ms/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 0.9596 - val_loss: 0.1484\nEpoch 9/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 835ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.9495 - val_loss: 0.1512\nEpoch 10/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 786ms/step - accuracy: 0.9998 - loss: 0.0079 - val_accuracy: 0.9596 - val_loss: 0.1390\nEpoch 11/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 847ms/step - accuracy: 1.0000 - loss: 0.0060 - val_accuracy: 0.9596 - val_loss: 0.1598\nEpoch 12/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 804ms/step - accuracy: 1.0000 - loss: 0.0038 - val_accuracy: 0.9596 - val_loss: 0.1435\nEpoch 13/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 779ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.9495 - val_loss: 0.1719\nEpoch 14/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 823ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.9495 - val_loss: 0.1676\nEpoch 15/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 841ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.9646 - val_loss: 0.1533\nEpoch 16/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 786ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.9646 - val_loss: 0.1433\nEpoch 17/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 806ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.9646 - val_loss: 0.1462\nEpoch 18/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 817ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.9646 - val_loss: 0.1483\nEpoch 19/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 816ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.9495 - val_loss: 0.1645\nEpoch 20/20\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 803ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.9545 - val_loss: 0.1617\n\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 567ms/step - accuracy: 0.9711 - loss: 0.0958\n\nFinal validation accuracy: 0.9545\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"top_model.save(\"top_classifier_head.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T13:01:11.711878Z","iopub.execute_input":"2025-07-04T13:01:11.712576Z","iopub.status.idle":"2025-07-04T13:01:11.727283Z","shell.execute_reply.started":"2025-07-04T13:01:11.712544Z","shell.execute_reply":"2025-07-04T13:01:11.726226Z"}},"outputs":[],"execution_count":3}]}